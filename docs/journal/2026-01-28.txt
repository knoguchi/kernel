# Kenix Journal - 2026-01-28

## Shared Memory IPC and 4KB Page Support

### Goals for Today
- Implement 4KB page granularity (in addition to existing 2MB blocks)
- Implement shared memory (SHM) syscalls for high-bandwidth IPC
- Debug and fix init task crash

---

## Part 1: 4KB Page Support

### Background

The existing MMU setup used only 2MB block descriptors (L2 level), which is
wasteful for small shared memory regions. To support practical shared buffers,
we need 4KB page granularity via L3 page tables.

### AArch64 Page Table Hierarchy

```
L1 (512 entries x 1GB each)
 └── L2 (512 entries x 2MB each)
      ├── 2MB BLOCK descriptor (for large mappings)
      └── L3 TABLE descriptor → L3 (512 entries x 4KB each) → 4KB PAGE descriptor
```

### Page Table Entry Types

```
L2 Entry Types (bits [1:0]):
  0b01 = Block descriptor (2MB mapping)
  0b11 = Table descriptor (points to L3)

L3 Entry Type (bits [1:0]):
  0b11 = Page descriptor (4KB mapping)
```

### Implementation in address_space.rs

Added L3 table tracking and 4KB mapping functions:

```rust
pub struct AddressSpace {
    ttbr0: PhysAddr,
    l2_tables: [Option<PhysAddr>; MAX_L1_ENTRIES],
    /// L3 tables: indexed by l1_idx * 512 + l2_idx
    l3_tables: [Option<PhysAddr>; MAX_L1_ENTRIES * ENTRIES_PER_TABLE],
}

impl AddressSpace {
    /// Map a single 4KB page
    pub unsafe fn map_4kb(&mut self, vaddr: usize, paddr: PhysAddr, flags: PageFlags) -> bool {
        let l1_idx = l1_index(vaddr);
        let l2_idx = l2_index(vaddr);
        let l3_idx = l3_index(vaddr);

        // Ensure L2 table exists
        // Check if L2 entry is already a 2MB block (can't mix)
        // Allocate L3 table if needed
        // Update L2 entry to point to L3 table
        // Write L3 page entry
        // ...
    }
}
```

### Virtual Address Bit Layout (4KB granule)

```
63                48 47          39 38          30 29          21 20          12 11           0
┌──────────────────┬──────────────┬──────────────┬──────────────┬──────────────┬──────────────┐
│     Reserved     │   L1 index   │   L2 index   │   L3 index   │  Page offset │              │
│    (sign ext)    │   (9 bits)   │   (9 bits)   │   (9 bits)   │   (12 bits)  │              │
└──────────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┘
```

Index extraction:
```rust
pub fn l1_index(va: usize) -> usize { (va >> 30) & 0x1FF }
pub fn l2_index(va: usize) -> usize { (va >> 21) & 0x1FF }
pub fn l3_index(va: usize) -> usize { (va >> 12) & 0x1FF }
```

---

## Part 2: Shared Memory Syscalls

### Syscall Interface

| Syscall | Number | Args | Returns | Description |
|---------|--------|------|---------|-------------|
| `sys_shmcreate` | 10 | x0=size | shm_id or error | Create shared memory region |
| `sys_shmmap` | 11 | x0=shm_id, x1=vaddr (0=auto) | vaddr or error | Map into current task |
| `sys_shmunmap` | 12 | x0=shm_id | 0 or error | Unmap from current task |
| `sys_shmgrant` | 13 | x0=shm_id, x1=task_id | 0 or error | Allow task to map region |

### Shared Memory Region Structure (kernel/src/shm.rs)

```rust
pub struct ShmRegion {
    pub in_use: bool,
    pub id: ShmId,
    pub frames: [Option<PhysAddr>; MAX_SHM_PAGES],  // Physical frames (4KB each)
    pub num_frames: usize,
    pub size: usize,
    pub owner: TaskId,
    pub granted: [bool; MAX_TASKS],                 // Access permissions
    pub mapped_vaddr: [Option<usize>; MAX_TASKS],   // Mapped vaddr per task
}

pub static mut SHM_REGIONS: [ShmRegion; MAX_SHM_REGIONS];
```

### User-space API (user/shm.h)

```c
typedef unsigned long ShmId;

static inline ShmId sys_shmcreate(unsigned long size);
static inline void* sys_shmmap(ShmId id, void *hint);
static inline long sys_shmunmap(ShmId id);
static inline long sys_shmgrant(ShmId id, unsigned long task_id);
```

### Console Server SHM Support

The console server (user/console.c) handles MSG_SHM_WRITE messages:

```c
} else if (recv.msg.tag == MSG_SHM_WRITE) {
    // MSG_SHM_WRITE: data[0]=shm_id, data[1]=offset, data[2]=len
    ShmId shm_id = recv.msg.data[0];
    unsigned long offset = recv.msg.data[1];
    unsigned long len = recv.msg.data[2];

    // Get mapped address for this client's SHM
    void *shm_base = get_client_shm(recv.sender, shm_id);

    // Write from SHM to UART
    const char *buf = (const char*)shm_base + offset;
    uart_write(buf, len);
    // ...
}
```

---

## Part 3: The Stack Overflow Bug

### Symptoms

After implementing SHM, the init task crashed with "Unknown exception" at
PC 0x12c000 before printing anything. Console server worked fine.

### Investigation

Added extensive debug output to trace ELF loading. Found that:
1. `phdr.p_filesz` was being read as 0 instead of 204
2. Memory at the phdr location appeared corrupted
3. Non-volatile reads showed "correct" cached values
4. Volatile reads revealed actual memory corruption

### Root Cause

Examining linker symbols revealed the problem:

```
Symbol                  Address
__init_elf_end          0x4010f9f8
_stack_bottom           0x4010fa00
```

The gap between embedded init ELF and stack bottom was only **8 bytes**!

With a 64KB stack, heavy function calls during ELF loading caused stack
overflow that corrupted the embedded init ELF binary data.

### The Fix

Increased kernel stack from 64KB to 256KB in kernel/linker.ld:

```ld
/* Stack: 256KB, grows downward (increased from 64KB to prevent overflow) */
. = ALIGN(16);
_stack_bottom = .;
. += 256K;
_stack_top = .;
```

### Memory Layout After Fix

```
.user_code section:
  __console_elf_start
  ... (console.elf embedded)
  __console_elf_end
  __init_elf_start
  ... (init.elf embedded)
  __init_elf_end          @ 0x4010f9f8

_stack_bottom             @ 0x40150000  (after ALIGN + 256K gap)
... (256KB stack space)
_stack_top                @ 0x40190000
```

---

## Part 4: Working System Output

After all fixes, the system output:

```
Initializing scheduler...
  Created idle task (id=0)

Creating console server...
  Console ELF at 0x400ee000, size 67104 bytes
  Created console server (id=1) - runs in EL0 with UART access

Creating init task...
  Init ELF at 0x400fe620, size 67744 bytes
  Entry point: 0x00000000
  PT_LOAD segments: 2
    Segment 0: vaddr=0x00000000, filesz=1072, memsz=1072, flags=R-X
    Segment 1: vaddr=0x00000430, filesz=246, memsz=246, flags=R--
  Created init task (id=2) - runs in EL0

Starting scheduler...
[console] Server started
Hello via IPC!
Init running in EL0
IPC works!
This is a much longer string that exceeds the 24-byte inline limit and demonstrates shared memory IPC working correctly!
Shared memory enables efficient transfer of large data between tasks without copying through the kernel message registers.
SHM test complete!
```

---

## Summary

### What Works Now

1. **Inline IPC**: Messages with up to 24 bytes of inline data
2. **Shared Memory IPC**: Large data transfer via mapped shared regions
3. **4KB Page Support**: Infrastructure ready for fine-grained mappings
4. **User-space Tasks**: Console server and init both run in EL0
5. **Preemptive Scheduling**: Timer-based round-robin with IPC blocking

### Files Changed

| File | Changes |
|------|---------|
| kernel/src/mm/paging.rs | Added l3_index() function |
| kernel/src/mm/address_space.rs | Added L3 tables, map_4kb(), unmap_4kb() |
| kernel/src/shm.rs | New: SHM syscall implementations |
| kernel/src/syscall.rs | Added SHM syscall handlers (10-13) |
| kernel/src/main.rs | Added mod shm |
| kernel/linker.ld | Increased stack to 256KB |
| user/shm.h | New: User-space SHM wrappers |
| user/ipc.h | Added MSG_SHM_WRITE constant |
| user/console.c | Added MSG_SHM_WRITE handling |
| user/init.c | Added SHM test code |

### Lessons Learned

1. **Volatile reads matter**: Non-volatile reads can show stale cached values
   when memory is being corrupted by stack overflow.

2. **Check stack proximity**: When embedding data in the binary, verify
   there's adequate distance between data and stack regions.

3. **Debug output is double-edged**: Adding debug prints can mask timing-
   dependent bugs or change stack usage patterns.

---

## Next Steps

- Implement proper vaddr allocation for SHM regions
- Add SHM reference counting and cleanup on task exit
- Consider copy-on-write (COW) for efficient fork()
- Filesystem server (VFS)
- Block device driver
